[2024-12-14T16:36:22.649+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-12-14T16:36:22.697+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: migration_fit_data.aggregate_warehouse.aggregate_course_performance manual__2024-12-14T16:34:48.600629+00:00 [queued]>
[2024-12-14T16:36:22.710+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: migration_fit_data.aggregate_warehouse.aggregate_course_performance manual__2024-12-14T16:34:48.600629+00:00 [queued]>
[2024-12-14T16:36:22.710+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-12-14T16:36:22.727+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): aggregate_warehouse.aggregate_course_performance> on 2024-12-14 16:34:48.600629+00:00
[2024-12-14T16:36:22.740+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=4988) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-12-14T16:36:22.742+0000] {standard_task_runner.py:63} INFO - Started process 4998 to run task
[2024-12-14T16:36:22.745+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'migration_fit_data', 'aggregate_warehouse.aggregate_course_performance', 'manual__2024-12-14T16:34:48.600629+00:00', '--job-id', '281', '--raw', '--subdir', 'DAGS_FOLDER/elt_migrate.py', '--cfg-path', '/tmp/tmphmqmq0tm']
[2024-12-14T16:36:22.751+0000] {standard_task_runner.py:91} INFO - Job 281: Subtask aggregate_warehouse.aggregate_course_performance
[2024-12-14T16:36:22.900+0000] {task_command.py:426} INFO - Running <TaskInstance: migration_fit_data.aggregate_warehouse.aggregate_course_performance manual__2024-12-14T16:34:48.600629+00:00 [running]> on host 472b63b715e1
[2024-12-14T16:36:23.113+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='migration_fit_data' AIRFLOW_CTX_TASK_ID='aggregate_warehouse.aggregate_course_performance' AIRFLOW_CTX_EXECUTION_DATE='2024-12-14T16:34:48.600629+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-14T16:34:48.600629+00:00'
[2024-12-14T16:36:23.115+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-12-14T16:36:23.141+0000] {hive.py:475} INFO - USE `default`
[2024-12-14T16:36:23.252+0000] {hive.py:475} INFO - CREATE NAMESPACE IF NOT EXISTS iceberg.aggr_warehouse
[2024-12-14T16:36:23.288+0000] {hive.py:475} INFO - DROP TABLE IF EXISTS iceberg.aggr_warehouse.course_performance
[2024-12-14T16:36:23.409+0000] {hive.py:475} INFO - 
    CREATE OR REPLACE TABLE iceberg.aggr_warehouse.course_performance
    USING parquet
    AS
    WITH unique_scores AS (
        SELECT
            c.course_code,
            c.course_name,
            ps.semester_name,
            sf.student_id,
            sf.final_score
        FROM
            iceberg.warehouse.course c
        JOIN iceberg.warehouse.instruction_fact inf 
            ON c.course_id = inf.course_id
        JOIN iceberg.warehouse.program_semester ps 
            ON inf.program_semester_id = ps.program_semester_id
        JOIN iceberg.warehouse.score_fact sf 
            ON sf.instruction_id = inf.instruction_id
        WHERE 
            inf.instruction_status = 'completed'
        GROUP BY
            c.course_id, c.course_name, ps.semester_name, sf.student_id, sf.final_score
    )
    SELECT
        course_code,
        course_name,
        semester_name,
        COUNT (student_id) AS num_students_enrolled,
        SUM(CASE WHEN final_score >= 4 THEN 1 ELSE 0 END) AS num_students_passed,
        (SUM(CASE WHEN final_score >= 4 THEN 1 ELSE 0 END) * 100.0) / COUNT(student_id) AS pass_rate,
        AVG(final_score) AS average_final_score
    FROM
        unique_scores
    GROUP BY
        course_code, course_name, semester_name;
    
[2024-12-14T16:36:23.548+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-12-14T16:36:23.550+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/tasks/fit_task.py", line 137, in aggregate_into_warehouse
    cursor.execute(FIT_AGGREGATE_TABLES[table]["create_table_command"])
  File "/home/airflow/.local/lib/python3.12/site-packages/pyhive/hive.py", line 481, in execute
    _check_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/pyhive/hive.py", line 611, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error running query: [MISSING_AGGREGATION] org.apache.spark.sql.AnalysisException: [MISSING_AGGREGATION] The non-aggregating expression "course_code" is based on columns which are not participating in the GROUP BY clause.\nAdd the columns or the expression to the GROUP BY, aggregate the expression, or use "any_value(course_code)" if you do not care which of the values within a group is returned.:36:35', 'org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$:runningQueryError:HiveThriftServerErrors.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:262', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:runInternal:SparkExecuteStatementOperation.scala:152', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:277', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkOperation$$super$run:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:$anonfun$run$1:SparkOperation.scala:45', 'scala.runtime.java8.JFunction0$mcV$sp:apply:JFunction0$mcV$sp.java:23', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties:SparkOperation.scala:79', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties$:SparkOperation.scala:63', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:withLocalProperties:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run:SparkOperation.scala:45', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run$:SparkOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:run:SparkExecuteStatementOperation.scala:41', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:484', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:460', 'jdk.internal.reflect.GeneratedMethodAccessor16:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke::-1', 'java.lang.reflect.Method:invoke::-1', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:71', 'org.apache.hive.service.cli.session.HiveSessionProxy:lambda$invoke$0:HiveSessionProxy.java:58', 'java.security.AccessController:doPrivileged::-2', 'javax.security.auth.Subject:doAs::-1', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:58', 'com.sun.proxy.$Proxy39:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:282', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:453', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:52', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:310', 'java.util.concurrent.ThreadPoolExecutor:runWorker::-1', 'java.util.concurrent.ThreadPoolExecutor$Worker:run::-1', 'java.lang.Thread:run::-1', '*org.apache.spark.sql.AnalysisException:[MISSING_AGGREGATION] The non-aggregating expression "course_code" is based on columns which are not participating in the GROUP BY clause.\nAdd the columns or the expression to the GROUP BY, aggregate the expression, or use "any_value(course_code)" if you do not care which of the values within a group is returned.:123:88', 'org.apache.spark.sql.errors.QueryCompilationErrors$:columnNotInGroupByClauseError:QueryCompilationErrors.scala:3445', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkValidAggregateExpression$1:CheckAnalysis.scala:439', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis0$22:CheckAnalysis.scala:485', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis0$22$adapted:CheckAnalysis.scala:485', 'scala.collection.immutable.List:foreach:List.scala:431', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis0$2:CheckAnalysis.scala:485', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis0$2$adapted:CheckAnalysis.scala:182', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:244', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:243', 'scala.collection.Iterator:foreach:Iterator.scala:943', 'scala.collection.Iterator:foreach$:Iterator.scala:943', 'scala.collection.AbstractIterator:foreach:Iterator.scala:1431', 'scala.collection.IterableLike:foreach:IterableLike.scala:74', 'scala.collection.IterableLike:foreach$:IterableLike.scala:73', 'scala.collection.AbstractIterable:foreach:Iterable.scala:56', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:243', 'scala.collection.Iterator:foreach:Iterator.scala:943', 'scala.collection.Iterator:foreach$:Iterator.scala:943', 'scala.collection.AbstractIterator:foreach:Iterator.scala:1431', 'scala.collection.IterableLike:foreach:IterableLike.scala:74', 'scala.collection.IterableLike:foreach$:IterableLike.scala:73', 'scala.collection.AbstractIterable:foreach:Iterable.scala:56', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:243', 'scala.collection.immutable.List:foreach:List.scala:431', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis0:CheckAnalysis.scala:182', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis0$:CheckAnalysis.scala:164', 'org.apache.spark.sql.catalyst.analysis.Analyzer:checkAnalysis0:Analyzer.scala:188', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis:CheckAnalysis.scala:160', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis$:CheckAnalysis.scala:150', 'org.apache.spark.sql.catalyst.analysis.Analyzer:checkAnalysis:Analyzer.scala:188', 'org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$$anonfun$apply$59:applyOrElse:Analyzer.scala:3743', 'org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$$anonfun$apply$59:applyOrElse:Analyzer.scala:3741', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsDownWithPruning$2:AnalysisHelper.scala:170', 'org.apache.spark.sql.catalyst.trees.CurrentOrigin$:withOrigin:origin.scala:76', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsDownWithPruning$1:AnalysisHelper.scala:170', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsDownWithPruning:AnalysisHelper.scala:168', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsDownWithPruning$:AnalysisHelper.scala:164', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsDownWithPruning:LogicalPlan.scala:32', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsWithPruning:AnalysisHelper.scala:99', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsWithPruning$:AnalysisHelper.scala:96', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsWithPruning:LogicalPlan.scala:32', 'org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$:apply:Analyzer.scala:3741', 'org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$:apply:Analyzer.scala:3739', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$2:RuleExecutor.scala:222', 'scala.collection.IndexedSeqOptimized:foldLeft:IndexedSeqOptimized.scala:60', 'scala.collection.IndexedSeqOptimized:foldLeft$:IndexedSeqOptimized.scala:68', 'scala.collection.mutable.WrappedArray:foldLeft:WrappedArray.scala:38', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1:RuleExecutor.scala:219', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1$adapted:RuleExecutor.scala:211', 'scala.collection.immutable.List:foreach:List.scala:431', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:execute:RuleExecutor.scala:211', 'org.apache.spark.sql.catalyst.analysis.Analyzer:org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext:Analyzer.scala:226', 'org.apache.spark.sql.catalyst.analysis.Analyzer:$anonfun$execute$1:Analyzer.scala:222', 'org.apache.spark.sql.catalyst.analysis.AnalysisContext$:withNewAnalysisContext:Analyzer.scala:173', 'org.apache.spark.sql.catalyst.analysis.Analyzer:execute:Analyzer.scala:222', 'org.apache.spark.sql.catalyst.analysis.Analyzer:execute:Analyzer.scala:188', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$executeAndTrack$1:RuleExecutor.scala:182', 'org.apache.spark.sql.catalyst.QueryPlanningTracker$:withTracker:QueryPlanningTracker.scala:89', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:executeAndTrack:RuleExecutor.scala:182', 'org.apache.spark.sql.catalyst.analysis.Analyzer:$anonfun$executeAndCheck$1:Analyzer.scala:209', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:markInAnalyzer:AnalysisHelper.scala:330', 'org.apache.spark.sql.catalyst.analysis.Analyzer:executeAndCheck:Analyzer.scala:208', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$analyzed$1:QueryExecution.scala:77', 'org.apache.spark.sql.catalyst.QueryPlanningTracker:measurePhase:QueryPlanningTracker.scala:138', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$2:QueryExecution.scala:219', 'org.apache.spark.sql.execution.QueryExecution$:withInternalError:QueryExecution.scala:546', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$1:QueryExecution.scala:219', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.execution.QueryExecution:executePhase:QueryExecution.scala:218', 'org.apache.spark.sql.execution.QueryExecution:analyzed$lzycompute:QueryExecution.scala:77', 'org.apache.spark.sql.execution.QueryExecution:analyzed:QueryExecution.scala:74', 'org.apache.spark.sql.execution.QueryExecution:assertAnalyzed:QueryExecution.scala:66', 'org.apache.spark.sql.Dataset$:$anonfun$ofRows$2:Dataset.scala:99', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.Dataset$:ofRows:Dataset.scala:97', 'org.apache.spark.sql.SparkSession:$anonfun$sql$4:SparkSession.scala:691', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:682', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:713', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:744', 'org.apache.spark.sql.SQLContext:sql:SQLContext.scala:651', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:227'], sqlState='42803', errorCode=0, errorMessage='Error running query: [MISSING_AGGREGATION] org.apache.spark.sql.AnalysisException: [MISSING_AGGREGATION] The non-aggregating expression "course_code" is based on columns which are not participating in the GROUP BY clause.\nAdd the columns or the expression to the GROUP BY, aggregate the expression, or use "any_value(course_code)" if you do not care which of the values within a group is returned.'), operationHandle=None)
[2024-12-14T16:36:23.573+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=migration_fit_data, task_id=aggregate_warehouse.aggregate_course_performance, run_id=manual__2024-12-14T16:34:48.600629+00:00, execution_date=20241214T163448, start_date=20241214T163622, end_date=20241214T163623
[2024-12-14T16:36:23.593+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 281 for task aggregate_warehouse.aggregate_course_performance (TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error running query: [MISSING_AGGREGATION] org.apache.spark.sql.AnalysisException: [MISSING_AGGREGATION] The non-aggregating expression "course_code" is based on columns which are not participating in the GROUP BY clause.\nAdd the columns or the expression to the GROUP BY, aggregate the expression, or use "any_value(course_code)" if you do not care which of the values within a group is returned.:36:35', 'org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$:runningQueryError:HiveThriftServerErrors.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:262', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:runInternal:SparkExecuteStatementOperation.scala:152', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:277', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkOperation$$super$run:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:$anonfun$run$1:SparkOperation.scala:45', 'scala.runtime.java8.JFunction0$mcV$sp:apply:JFunction0$mcV$sp.java:23', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties:SparkOperation.scala:79', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties$:SparkOperation.scala:63', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:withLocalProperties:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run:SparkOperation.scala:45', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run$:SparkOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:run:SparkExecuteStatementOperation.scala:41', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:484', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:460', 'jdk.internal.reflect.GeneratedMethodAccessor16:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke::-1', 'java.lang.reflect.Method:invoke::-1', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:71', 'org.apache.hive.service.cli.session.HiveSessionProxy:lambda$invoke$0:HiveSessionProxy.java:58', 'java.security.AccessController:doPrivileged::-2', 'javax.security.auth.Subject:doAs::-1', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:58', 'com.sun.proxy.$Proxy39:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:282', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:453', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:52', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:310', 'java.util.concurrent.ThreadPoolExecutor:runWorker::-1', 'java.util.concurrent.ThreadPoolExecutor$Worker:run::-1', 'java.lang.Thread:run::-1', '*org.apache.spark.sql.AnalysisException:[MISSING_AGGREGATION] The non-aggregating expression "course_code" is based on columns which are not participating in the GROUP BY clause.\nAdd the columns or the expression to the GROUP BY, aggregate the expression, or use "any_value(course_code)" if you do not care which of the values within a group is returned.:123:88', 'org.apache.spark.sql.errors.QueryCompilationErrors$:columnNotInGroupByClauseError:QueryCompilationErrors.scala:3445', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkValidAggregateExpression$1:CheckAnalysis.scala:439', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis0$22:CheckAnalysis.scala:485', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis0$22$adapted:CheckAnalysis.scala:485', 'scala.collection.immutable.List:foreach:List.scala:431', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis0$2:CheckAnalysis.scala:485', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:$anonfun$checkAnalysis0$2$adapted:CheckAnalysis.scala:182', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:244', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:243', 'scala.collection.Iterator:foreach:Iterator.scala:943', 'scala.collection.Iterator:foreach$:Iterator.scala:943', 'scala.collection.AbstractIterator:foreach:Iterator.scala:1431', 'scala.collection.IterableLike:foreach:IterableLike.scala:74', 'scala.collection.IterableLike:foreach$:IterableLike.scala:73', 'scala.collection.AbstractIterable:foreach:Iterable.scala:56', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:243', 'scala.collection.Iterator:foreach:Iterator.scala:943', 'scala.collection.Iterator:foreach$:Iterator.scala:943', 'scala.collection.AbstractIterator:foreach:Iterator.scala:1431', 'scala.collection.IterableLike:foreach:IterableLike.scala:74', 'scala.collection.IterableLike:foreach$:IterableLike.scala:73', 'scala.collection.AbstractIterable:foreach:Iterable.scala:56', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$foreachUp$1$adapted:TreeNode.scala:243', 'scala.collection.immutable.List:foreach:List.scala:431', 'org.apache.spark.sql.catalyst.trees.TreeNode:foreachUp:TreeNode.scala:243', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis0:CheckAnalysis.scala:182', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis0$:CheckAnalysis.scala:164', 'org.apache.spark.sql.catalyst.analysis.Analyzer:checkAnalysis0:Analyzer.scala:188', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis:CheckAnalysis.scala:160', 'org.apache.spark.sql.catalyst.analysis.CheckAnalysis:checkAnalysis$:CheckAnalysis.scala:150', 'org.apache.spark.sql.catalyst.analysis.Analyzer:checkAnalysis:Analyzer.scala:188', 'org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$$anonfun$apply$59:applyOrElse:Analyzer.scala:3743', 'org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$$anonfun$apply$59:applyOrElse:Analyzer.scala:3741', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsDownWithPruning$2:AnalysisHelper.scala:170', 'org.apache.spark.sql.catalyst.trees.CurrentOrigin$:withOrigin:origin.scala:76', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsDownWithPruning$1:AnalysisHelper.scala:170', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsDownWithPruning:AnalysisHelper.scala:168', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsDownWithPruning$:AnalysisHelper.scala:164', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsDownWithPruning:LogicalPlan.scala:32', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsWithPruning:AnalysisHelper.scala:99', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsWithPruning$:AnalysisHelper.scala:96', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsWithPruning:LogicalPlan.scala:32', 'org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$:apply:Analyzer.scala:3741', 'org.apache.spark.sql.catalyst.analysis.Analyzer$HandleSpecialCommand$:apply:Analyzer.scala:3739', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$2:RuleExecutor.scala:222', 'scala.collection.IndexedSeqOptimized:foldLeft:IndexedSeqOptimized.scala:60', 'scala.collection.IndexedSeqOptimized:foldLeft$:IndexedSeqOptimized.scala:68', 'scala.collection.mutable.WrappedArray:foldLeft:WrappedArray.scala:38', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1:RuleExecutor.scala:219', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1$adapted:RuleExecutor.scala:211', 'scala.collection.immutable.List:foreach:List.scala:431', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:execute:RuleExecutor.scala:211', 'org.apache.spark.sql.catalyst.analysis.Analyzer:org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext:Analyzer.scala:226', 'org.apache.spark.sql.catalyst.analysis.Analyzer:$anonfun$execute$1:Analyzer.scala:222', 'org.apache.spark.sql.catalyst.analysis.AnalysisContext$:withNewAnalysisContext:Analyzer.scala:173', 'org.apache.spark.sql.catalyst.analysis.Analyzer:execute:Analyzer.scala:222', 'org.apache.spark.sql.catalyst.analysis.Analyzer:execute:Analyzer.scala:188', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$executeAndTrack$1:RuleExecutor.scala:182', 'org.apache.spark.sql.catalyst.QueryPlanningTracker$:withTracker:QueryPlanningTracker.scala:89', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:executeAndTrack:RuleExecutor.scala:182', 'org.apache.spark.sql.catalyst.analysis.Analyzer:$anonfun$executeAndCheck$1:Analyzer.scala:209', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:markInAnalyzer:AnalysisHelper.scala:330', 'org.apache.spark.sql.catalyst.analysis.Analyzer:executeAndCheck:Analyzer.scala:208', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$analyzed$1:QueryExecution.scala:77', 'org.apache.spark.sql.catalyst.QueryPlanningTracker:measurePhase:QueryPlanningTracker.scala:138', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$2:QueryExecution.scala:219', 'org.apache.spark.sql.execution.QueryExecution$:withInternalError:QueryExecution.scala:546', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$1:QueryExecution.scala:219', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.execution.QueryExecution:executePhase:QueryExecution.scala:218', 'org.apache.spark.sql.execution.QueryExecution:analyzed$lzycompute:QueryExecution.scala:77', 'org.apache.spark.sql.execution.QueryExecution:analyzed:QueryExecution.scala:74', 'org.apache.spark.sql.execution.QueryExecution:assertAnalyzed:QueryExecution.scala:66', 'org.apache.spark.sql.Dataset$:$anonfun$ofRows$2:Dataset.scala:99', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.Dataset$:ofRows:Dataset.scala:97', 'org.apache.spark.sql.SparkSession:$anonfun$sql$4:SparkSession.scala:691', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:682', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:713', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:744', 'org.apache.spark.sql.SQLContext:sql:SQLContext.scala:651', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:227'], sqlState='42803', errorCode=0, errorMessage='Error running query: [MISSING_AGGREGATION] org.apache.spark.sql.AnalysisException: [MISSING_AGGREGATION] The non-aggregating expression "course_code" is based on columns which are not participating in the GROUP BY clause.\nAdd the columns or the expression to the GROUP BY, aggregate the expression, or use "any_value(course_code)" if you do not care which of the values within a group is returned.'), operationHandle=None); 4998)
[2024-12-14T16:36:23.622+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2024-12-14T16:36:23.683+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-12-14T16:36:23.697+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
