[2024-12-14T08:51:45.191+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-12-14T08:51:45.271+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: migration_fit_data.create_dwh_table.recreate_dwh_score_fact manual__2024-12-14T08:50:05.480259+00:00 [queued]>
[2024-12-14T08:51:45.312+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: migration_fit_data.create_dwh_table.recreate_dwh_score_fact manual__2024-12-14T08:50:05.480259+00:00 [queued]>
[2024-12-14T08:51:45.317+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-12-14T08:51:45.389+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): create_dwh_table.recreate_dwh_score_fact> on 2024-12-14 08:50:05.480259+00:00
[2024-12-14T08:51:45.428+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=16341) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-12-14T08:51:45.441+0000] {standard_task_runner.py:63} INFO - Started process 16379 to run task
[2024-12-14T08:51:45.445+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'migration_fit_data', 'create_dwh_table.recreate_dwh_score_fact', 'manual__2024-12-14T08:50:05.480259+00:00', '--job-id', '1132', '--raw', '--subdir', 'DAGS_FOLDER/elt_migrate.py', '--cfg-path', '/tmp/tmpllkz5xfs']
[2024-12-14T08:51:45.454+0000] {standard_task_runner.py:91} INFO - Job 1132: Subtask create_dwh_table.recreate_dwh_score_fact
[2024-12-14T08:51:45.698+0000] {task_command.py:426} INFO - Running <TaskInstance: migration_fit_data.create_dwh_table.recreate_dwh_score_fact manual__2024-12-14T08:50:05.480259+00:00 [running]> on host 1fe973f28a1c
[2024-12-14T08:51:46.442+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='migration_fit_data' AIRFLOW_CTX_TASK_ID='create_dwh_table.recreate_dwh_score_fact' AIRFLOW_CTX_EXECUTION_DATE='2024-12-14T08:50:05.480259+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-14T08:50:05.480259+00:00'
[2024-12-14T08:51:46.444+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-12-14T08:51:46.470+0000] {hive.py:475} INFO - USE `default`
[2024-12-14T08:51:46.517+0000] {hive.py:475} INFO - DROP TABLE IF EXISTS iceberg.warehouse.score_fact
[2024-12-14T08:51:46.560+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-12-14T08:51:46.564+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/tasks/fit_task.py", line 75, in recreate_warehouse_table
    cursor.execute(f"DROP TABLE IF EXISTS {warehouse_table_name}")
  File "/home/airflow/.local/lib/python3.12/site-packages/pyhive/hive.py", line 481, in execute
    _check_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/pyhive/hive.py", line 611, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error running query: org.apache.iceberg.exceptions.NotFoundException: Failed to open input stream for file: hdfs://namenode:9000/fit_warehouse/warehouse/score_fact/*.snappy.parquet/metadata/00001-789b9ac6-4e78-4799-96e8-1cc41ccdb53e.metadata.json:36:35', 'org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$:runningQueryError:HiveThriftServerErrors.scala:46', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:262', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:runInternal:SparkExecuteStatementOperation.scala:152', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:277', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkOperation$$super$run:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:$anonfun$run$1:SparkOperation.scala:45', 'scala.runtime.java8.JFunction0$mcV$sp:apply:JFunction0$mcV$sp.java:23', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties:SparkOperation.scala:79', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties$:SparkOperation.scala:63', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:withLocalProperties:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run:SparkOperation.scala:45', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run$:SparkOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:run:SparkExecuteStatementOperation.scala:41', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:484', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:460', 'jdk.internal.reflect.GeneratedMethodAccessor20:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke::-1', 'java.lang.reflect.Method:invoke::-1', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:71', 'org.apache.hive.service.cli.session.HiveSessionProxy:lambda$invoke$0:HiveSessionProxy.java:58', 'java.security.AccessController:doPrivileged::-2', 'javax.security.auth.Subject:doAs::-1', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:58', 'com.sun.proxy.$Proxy39:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:282', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:453', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:52', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:310', 'java.util.concurrent.ThreadPoolExecutor:runWorker::-1', 'java.util.concurrent.ThreadPoolExecutor$Worker:run::-1', 'java.lang.Thread:run::-1', '*org.apache.iceberg.exceptions.NotFoundException:Failed to open input stream for file: hdfs://namenode:9000/fit_warehouse/warehouse/score_fact/*.snappy.parquet/metadata/00001-789b9ac6-4e78-4799-96e8-1cc41ccdb53e.metadata.json:95:60', 'org.apache.iceberg.hadoop.HadoopInputFile:newStream:HadoopInputFile.java:185', 'org.apache.iceberg.TableMetadataParser:read:TableMetadataParser.java:279', 'org.apache.iceberg.TableMetadataParser:read:TableMetadataParser.java:273', 'org.apache.iceberg.BaseMetastoreTableOperations:lambda$refreshFromMetadataLocation$0:BaseMetastoreTableOperations.java:189', 'org.apache.iceberg.BaseMetastoreTableOperations:lambda$refreshFromMetadataLocation$1:BaseMetastoreTableOperations.java:208', 'org.apache.iceberg.util.Tasks$Builder:runTaskWithRetry:Tasks.java:413', 'org.apache.iceberg.util.Tasks$Builder:runSingleThreaded:Tasks.java:219', 'org.apache.iceberg.util.Tasks$Builder:run:Tasks.java:203', 'org.apache.iceberg.util.Tasks$Builder:run:Tasks.java:196', 'org.apache.iceberg.BaseMetastoreTableOperations:refreshFromMetadataLocation:BaseMetastoreTableOperations.java:208', 'org.apache.iceberg.BaseMetastoreTableOperations:refreshFromMetadataLocation:BaseMetastoreTableOperations.java:185', 'org.apache.iceberg.BaseMetastoreTableOperations:refreshFromMetadataLocation:BaseMetastoreTableOperations.java:180', 'org.apache.iceberg.hive.HiveTableOperations:doRefresh:HiveTableOperations.java:166', 'org.apache.iceberg.BaseMetastoreTableOperations:refresh:BaseMetastoreTableOperations.java:97', 'org.apache.iceberg.BaseMetastoreTableOperations:current:BaseMetastoreTableOperations.java:80', 'org.apache.iceberg.BaseMetastoreCatalog:loadTable:BaseMetastoreCatalog.java:49', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache:lambda$doComputeIfAbsent$14:BoundedLocalCache.java:2406', 'java.util.concurrent.ConcurrentHashMap:compute::-1', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache:doComputeIfAbsent:BoundedLocalCache.java:2404', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache:computeIfAbsent:BoundedLocalCache.java:2387', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache:computeIfAbsent:LocalCache.java:108', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache:get:LocalManualCache.java:62', 'org.apache.iceberg.CachingCatalog:loadTable:CachingCatalog.java:166', 'org.apache.iceberg.spark.SparkCatalog:load:SparkCatalog.java:843', 'org.apache.iceberg.spark.SparkCatalog:loadTable:SparkCatalog.java:170', 'org.apache.spark.sql.connector.catalog.TableCatalog:tableExists:TableCatalog.java:164', 'org.apache.spark.sql.execution.datasources.v2.DropTableExec:run:DropTableExec.scala:36', 'org.apache.spark.sql.execution.datasources.v2.V2CommandExec:result$lzycompute:V2CommandExec.scala:43', 'org.apache.spark.sql.execution.datasources.v2.V2CommandExec:result:V2CommandExec.scala:43', 'org.apache.spark.sql.execution.datasources.v2.V2CommandExec:executeCollect:V2CommandExec.scala:49', 'org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1:$anonfun$applyOrElse$1:QueryExecution.scala:107', 'org.apache.spark.sql.execution.SQLExecution$:$anonfun$withNewExecutionId$6:SQLExecution.scala:125', 'org.apache.spark.sql.execution.SQLExecution$:withSQLConfPropagated:SQLExecution.scala:201', 'org.apache.spark.sql.execution.SQLExecution$:$anonfun$withNewExecutionId$1:SQLExecution.scala:108', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.execution.SQLExecution$:withNewExecutionId:SQLExecution.scala:66', 'org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1:applyOrElse:QueryExecution.scala:107', 'org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1:applyOrElse:QueryExecution.scala:98', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$transformDownWithPruning$1:TreeNode.scala:461', 'org.apache.spark.sql.catalyst.trees.CurrentOrigin$:withOrigin:origin.scala:76', 'org.apache.spark.sql.catalyst.trees.TreeNode:transformDownWithPruning:TreeNode.scala:461', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning:LogicalPlan.scala:32', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:transformDownWithPruning:AnalysisHelper.scala:267', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:transformDownWithPruning$:AnalysisHelper.scala:263', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:transformDownWithPruning:LogicalPlan.scala:32', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:transformDownWithPruning:LogicalPlan.scala:32', 'org.apache.spark.sql.catalyst.trees.TreeNode:transformDown:TreeNode.scala:437', 'org.apache.spark.sql.execution.QueryExecution:eagerlyExecuteCommands:QueryExecution.scala:98', 'org.apache.spark.sql.execution.QueryExecution:commandExecuted$lzycompute:QueryExecution.scala:85', 'org.apache.spark.sql.execution.QueryExecution:commandExecuted:QueryExecution.scala:83', 'org.apache.spark.sql.Dataset:<init>:Dataset.scala:220', 'org.apache.spark.sql.Dataset$:$anonfun$ofRows$2:Dataset.scala:100', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.Dataset$:ofRows:Dataset.scala:97', 'org.apache.spark.sql.SparkSession:$anonfun$sql$4:SparkSession.scala:691', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:682', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:713', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:744', 'org.apache.spark.sql.SQLContext:sql:SQLContext.scala:651', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:227', '*java.io.FileNotFoundException:File does not exist: /fit_warehouse/warehouse/score_fact/*.snappy.parquet/metadata/00001-789b9ac6-4e78-4799-96e8-1cc41ccdb53e.metadata.json\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2089)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:762)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:458)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n:110:15', 'jdk.internal.reflect.NativeConstructorAccessorImpl:newInstance0::-2', 'jdk.internal.reflect.NativeConstructorAccessorImpl:newInstance::-1', 'jdk.internal.reflect.DelegatingConstructorAccessorImpl:newInstance::-1', 'java.lang.reflect.Constructor:newInstance::-1', 'org.apache.hadoop.ipc.RemoteException:instantiateException:RemoteException.java:121', 'org.apache.hadoop.ipc.RemoteException:unwrapRemoteException:RemoteException.java:88', 'org.apache.hadoop.hdfs.DFSClient:callGetBlockLocations:DFSClient.java:902', 'org.apache.hadoop.hdfs.DFSClient:getLocatedBlocks:DFSClient.java:889', 'org.apache.hadoop.hdfs.DFSClient:getLocatedBlocks:DFSClient.java:878', 'org.apache.hadoop.hdfs.DFSClient:open:DFSClient.java:1046', 'org.apache.hadoop.hdfs.DistributedFileSystem$4:doCall:DistributedFileSystem.java:340', 'org.apache.hadoop.hdfs.DistributedFileSystem$4:doCall:DistributedFileSystem.java:336', 'org.apache.hadoop.fs.FileSystemLinkResolver:resolve:FileSystemLinkResolver.java:81', 'org.apache.hadoop.hdfs.DistributedFileSystem:open:DistributedFileSystem.java:353', 'org.apache.hadoop.fs.FileSystem:open:FileSystem.java:976', 'org.apache.iceberg.hadoop.HadoopInputFile:newStream:HadoopInputFile.java:183', '*org.apache.hadoop.ipc.RemoteException:File does not exist: /fit_warehouse/warehouse/score_fact/*.snappy.parquet/metadata/00001-789b9ac6-4e78-4799-96e8-1cc41ccdb53e.metadata.json\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2089)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:762)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:458)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n:120:16', 'org.apache.hadoop.ipc.Client:getRpcResponse:Client.java:1612', 'org.apache.hadoop.ipc.Client:call:Client.java:1558', 'org.apache.hadoop.ipc.Client:call:Client.java:1455', 'org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke:ProtobufRpcEngine2.java:242', 'org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke:ProtobufRpcEngine2.java:129', 'com.sun.proxy.$Proxy34:getBlockLocations::-1', 'org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB:getBlockLocations:ClientNamenodeProtocolTranslatorPB.java:333', 'jdk.internal.reflect.GeneratedMethodAccessor80:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke::-1', 'java.lang.reflect.Method:invoke::-1', 'org.apache.hadoop.io.retry.RetryInvocationHandler:invokeMethod:RetryInvocationHandler.java:422', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeMethod:RetryInvocationHandler.java:165', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invoke:RetryInvocationHandler.java:157', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeOnce:RetryInvocationHandler.java:95', 'org.apache.hadoop.io.retry.RetryInvocationHandler:invoke:RetryInvocationHandler.java:359', 'com.sun.proxy.$Proxy35:getBlockLocations::-1', 'org.apache.hadoop.hdfs.DFSClient:callGetBlockLocations:DFSClient.java:900'], sqlState=None, errorCode=0, errorMessage='Error running query: org.apache.iceberg.exceptions.NotFoundException: Failed to open input stream for file: hdfs://namenode:9000/fit_warehouse/warehouse/score_fact/*.snappy.parquet/metadata/00001-789b9ac6-4e78-4799-96e8-1cc41ccdb53e.metadata.json'), operationHandle=None)
[2024-12-14T08:51:46.595+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=migration_fit_data, task_id=create_dwh_table.recreate_dwh_score_fact, run_id=manual__2024-12-14T08:50:05.480259+00:00, execution_date=20241214T085005, start_date=20241214T085145, end_date=20241214T085146
[2024-12-14T08:51:46.660+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 1132 for task create_dwh_table.recreate_dwh_score_fact (TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error running query: org.apache.iceberg.exceptions.NotFoundException: Failed to open input stream for file: hdfs://namenode:9000/fit_warehouse/warehouse/score_fact/*.snappy.parquet/metadata/00001-789b9ac6-4e78-4799-96e8-1cc41ccdb53e.metadata.json:36:35', 'org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$:runningQueryError:HiveThriftServerErrors.scala:46', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:262', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:runInternal:SparkExecuteStatementOperation.scala:152', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:277', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkOperation$$super$run:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:$anonfun$run$1:SparkOperation.scala:45', 'scala.runtime.java8.JFunction0$mcV$sp:apply:JFunction0$mcV$sp.java:23', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties:SparkOperation.scala:79', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties$:SparkOperation.scala:63', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:withLocalProperties:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run:SparkOperation.scala:45', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run$:SparkOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:run:SparkExecuteStatementOperation.scala:41', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:484', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:460', 'jdk.internal.reflect.GeneratedMethodAccessor20:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke::-1', 'java.lang.reflect.Method:invoke::-1', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:71', 'org.apache.hive.service.cli.session.HiveSessionProxy:lambda$invoke$0:HiveSessionProxy.java:58', 'java.security.AccessController:doPrivileged::-2', 'javax.security.auth.Subject:doAs::-1', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:58', 'com.sun.proxy.$Proxy39:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:282', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:453', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:52', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:310', 'java.util.concurrent.ThreadPoolExecutor:runWorker::-1', 'java.util.concurrent.ThreadPoolExecutor$Worker:run::-1', 'java.lang.Thread:run::-1', '*org.apache.iceberg.exceptions.NotFoundException:Failed to open input stream for file: hdfs://namenode:9000/fit_warehouse/warehouse/score_fact/*.snappy.parquet/metadata/00001-789b9ac6-4e78-4799-96e8-1cc41ccdb53e.metadata.json:95:60', 'org.apache.iceberg.hadoop.HadoopInputFile:newStream:HadoopInputFile.java:185', 'org.apache.iceberg.TableMetadataParser:read:TableMetadataParser.java:279', 'org.apache.iceberg.TableMetadataParser:read:TableMetadataParser.java:273', 'org.apache.iceberg.BaseMetastoreTableOperations:lambda$refreshFromMetadataLocation$0:BaseMetastoreTableOperations.java:189', 'org.apache.iceberg.BaseMetastoreTableOperations:lambda$refreshFromMetadataLocation$1:BaseMetastoreTableOperations.java:208', 'org.apache.iceberg.util.Tasks$Builder:runTaskWithRetry:Tasks.java:413', 'org.apache.iceberg.util.Tasks$Builder:runSingleThreaded:Tasks.java:219', 'org.apache.iceberg.util.Tasks$Builder:run:Tasks.java:203', 'org.apache.iceberg.util.Tasks$Builder:run:Tasks.java:196', 'org.apache.iceberg.BaseMetastoreTableOperations:refreshFromMetadataLocation:BaseMetastoreTableOperations.java:208', 'org.apache.iceberg.BaseMetastoreTableOperations:refreshFromMetadataLocation:BaseMetastoreTableOperations.java:185', 'org.apache.iceberg.BaseMetastoreTableOperations:refreshFromMetadataLocation:BaseMetastoreTableOperations.java:180', 'org.apache.iceberg.hive.HiveTableOperations:doRefresh:HiveTableOperations.java:166', 'org.apache.iceberg.BaseMetastoreTableOperations:refresh:BaseMetastoreTableOperations.java:97', 'org.apache.iceberg.BaseMetastoreTableOperations:current:BaseMetastoreTableOperations.java:80', 'org.apache.iceberg.BaseMetastoreCatalog:loadTable:BaseMetastoreCatalog.java:49', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache:lambda$doComputeIfAbsent$14:BoundedLocalCache.java:2406', 'java.util.concurrent.ConcurrentHashMap:compute::-1', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache:doComputeIfAbsent:BoundedLocalCache.java:2404', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache:computeIfAbsent:BoundedLocalCache.java:2387', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache:computeIfAbsent:LocalCache.java:108', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache:get:LocalManualCache.java:62', 'org.apache.iceberg.CachingCatalog:loadTable:CachingCatalog.java:166', 'org.apache.iceberg.spark.SparkCatalog:load:SparkCatalog.java:843', 'org.apache.iceberg.spark.SparkCatalog:loadTable:SparkCatalog.java:170', 'org.apache.spark.sql.connector.catalog.TableCatalog:tableExists:TableCatalog.java:164', 'org.apache.spark.sql.execution.datasources.v2.DropTableExec:run:DropTableExec.scala:36', 'org.apache.spark.sql.execution.datasources.v2.V2CommandExec:result$lzycompute:V2CommandExec.scala:43', 'org.apache.spark.sql.execution.datasources.v2.V2CommandExec:result:V2CommandExec.scala:43', 'org.apache.spark.sql.execution.datasources.v2.V2CommandExec:executeCollect:V2CommandExec.scala:49', 'org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1:$anonfun$applyOrElse$1:QueryExecution.scala:107', 'org.apache.spark.sql.execution.SQLExecution$:$anonfun$withNewExecutionId$6:SQLExecution.scala:125', 'org.apache.spark.sql.execution.SQLExecution$:withSQLConfPropagated:SQLExecution.scala:201', 'org.apache.spark.sql.execution.SQLExecution$:$anonfun$withNewExecutionId$1:SQLExecution.scala:108', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.execution.SQLExecution$:withNewExecutionId:SQLExecution.scala:66', 'org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1:applyOrElse:QueryExecution.scala:107', 'org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1:applyOrElse:QueryExecution.scala:98', 'org.apache.spark.sql.catalyst.trees.TreeNode:$anonfun$transformDownWithPruning$1:TreeNode.scala:461', 'org.apache.spark.sql.catalyst.trees.CurrentOrigin$:withOrigin:origin.scala:76', 'org.apache.spark.sql.catalyst.trees.TreeNode:transformDownWithPruning:TreeNode.scala:461', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning:LogicalPlan.scala:32', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:transformDownWithPruning:AnalysisHelper.scala:267', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:transformDownWithPruning$:AnalysisHelper.scala:263', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:transformDownWithPruning:LogicalPlan.scala:32', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:transformDownWithPruning:LogicalPlan.scala:32', 'org.apache.spark.sql.catalyst.trees.TreeNode:transformDown:TreeNode.scala:437', 'org.apache.spark.sql.execution.QueryExecution:eagerlyExecuteCommands:QueryExecution.scala:98', 'org.apache.spark.sql.execution.QueryExecution:commandExecuted$lzycompute:QueryExecution.scala:85', 'org.apache.spark.sql.execution.QueryExecution:commandExecuted:QueryExecution.scala:83', 'org.apache.spark.sql.Dataset:<init>:Dataset.scala:220', 'org.apache.spark.sql.Dataset$:$anonfun$ofRows$2:Dataset.scala:100', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.Dataset$:ofRows:Dataset.scala:97', 'org.apache.spark.sql.SparkSession:$anonfun$sql$4:SparkSession.scala:691', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:682', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:713', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:744', 'org.apache.spark.sql.SQLContext:sql:SQLContext.scala:651', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:227', '*java.io.FileNotFoundException:File does not exist: /fit_warehouse/warehouse/score_fact/*.snappy.parquet/metadata/00001-789b9ac6-4e78-4799-96e8-1cc41ccdb53e.metadata.json\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2089)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:762)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:458)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n:110:15', 'jdk.internal.reflect.NativeConstructorAccessorImpl:newInstance0::-2', 'jdk.internal.reflect.NativeConstructorAccessorImpl:newInstance::-1', 'jdk.internal.reflect.DelegatingConstructorAccessorImpl:newInstance::-1', 'java.lang.reflect.Constructor:newInstance::-1', 'org.apache.hadoop.ipc.RemoteException:instantiateException:RemoteException.java:121', 'org.apache.hadoop.ipc.RemoteException:unwrapRemoteException:RemoteException.java:88', 'org.apache.hadoop.hdfs.DFSClient:callGetBlockLocations:DFSClient.java:902', 'org.apache.hadoop.hdfs.DFSClient:getLocatedBlocks:DFSClient.java:889', 'org.apache.hadoop.hdfs.DFSClient:getLocatedBlocks:DFSClient.java:878', 'org.apache.hadoop.hdfs.DFSClient:open:DFSClient.java:1046', 'org.apache.hadoop.hdfs.DistributedFileSystem$4:doCall:DistributedFileSystem.java:340', 'org.apache.hadoop.hdfs.DistributedFileSystem$4:doCall:DistributedFileSystem.java:336', 'org.apache.hadoop.fs.FileSystemLinkResolver:resolve:FileSystemLinkResolver.java:81', 'org.apache.hadoop.hdfs.DistributedFileSystem:open:DistributedFileSystem.java:353', 'org.apache.hadoop.fs.FileSystem:open:FileSystem.java:976', 'org.apache.iceberg.hadoop.HadoopInputFile:newStream:HadoopInputFile.java:183', '*org.apache.hadoop.ipc.RemoteException:File does not exist: /fit_warehouse/warehouse/score_fact/*.snappy.parquet/metadata/00001-789b9ac6-4e78-4799-96e8-1cc41ccdb53e.metadata.json\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2089)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:762)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:458)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n:120:16', 'org.apache.hadoop.ipc.Client:getRpcResponse:Client.java:1612', 'org.apache.hadoop.ipc.Client:call:Client.java:1558', 'org.apache.hadoop.ipc.Client:call:Client.java:1455', 'org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke:ProtobufRpcEngine2.java:242', 'org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke:ProtobufRpcEngine2.java:129', 'com.sun.proxy.$Proxy34:getBlockLocations::-1', 'org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB:getBlockLocations:ClientNamenodeProtocolTranslatorPB.java:333', 'jdk.internal.reflect.GeneratedMethodAccessor80:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke::-1', 'java.lang.reflect.Method:invoke::-1', 'org.apache.hadoop.io.retry.RetryInvocationHandler:invokeMethod:RetryInvocationHandler.java:422', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeMethod:RetryInvocationHandler.java:165', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invoke:RetryInvocationHandler.java:157', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeOnce:RetryInvocationHandler.java:95', 'org.apache.hadoop.io.retry.RetryInvocationHandler:invoke:RetryInvocationHandler.java:359', 'com.sun.proxy.$Proxy35:getBlockLocations::-1', 'org.apache.hadoop.hdfs.DFSClient:callGetBlockLocations:DFSClient.java:900'], sqlState=None, errorCode=0, errorMessage='Error running query: org.apache.iceberg.exceptions.NotFoundException: Failed to open input stream for file: hdfs://namenode:9000/fit_warehouse/warehouse/score_fact/*.snappy.parquet/metadata/00001-789b9ac6-4e78-4799-96e8-1cc41ccdb53e.metadata.json'), operationHandle=None); 16379)
[2024-12-14T08:51:46.710+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2024-12-14T08:51:46.744+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-12-14T08:51:46.745+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
