[2024-12-14T11:27:30.716+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-12-14T11:27:30.740+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_instruction_fact_data.insert_warehouse.insert_dwh_instruction_fact manual__2024-12-14T11:27:22.467792+00:00 [queued]>
[2024-12-14T11:27:30.750+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_instruction_fact_data.insert_warehouse.insert_dwh_instruction_fact manual__2024-12-14T11:27:22.467792+00:00 [queued]>
[2024-12-14T11:27:30.751+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-12-14T11:27:30.767+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): insert_warehouse.insert_dwh_instruction_fact> on 2024-12-14 11:27:22.467792+00:00
[2024-12-14T11:27:30.777+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=789) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-12-14T11:27:30.779+0000] {standard_task_runner.py:63} INFO - Started process 793 to run task
[2024-12-14T11:27:30.782+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'etl_instruction_fact_data', 'insert_warehouse.insert_dwh_instruction_fact', 'manual__2024-12-14T11:27:22.467792+00:00', '--job-id', '165', '--raw', '--subdir', 'DAGS_FOLDER/elt_instruction_fact.py', '--cfg-path', '/tmp/tmpx1wrcvy0']
[2024-12-14T11:27:30.784+0000] {standard_task_runner.py:91} INFO - Job 165: Subtask insert_warehouse.insert_dwh_instruction_fact
[2024-12-14T11:27:30.833+0000] {task_command.py:426} INFO - Running <TaskInstance: etl_instruction_fact_data.insert_warehouse.insert_dwh_instruction_fact manual__2024-12-14T11:27:22.467792+00:00 [running]> on host 472b63b715e1
[2024-12-14T11:27:30.928+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='etl_instruction_fact_data' AIRFLOW_CTX_TASK_ID='insert_warehouse.insert_dwh_instruction_fact' AIRFLOW_CTX_EXECUTION_DATE='2024-12-14T11:27:22.467792+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-14T11:27:22.467792+00:00'
[2024-12-14T11:27:30.929+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-12-14T11:27:30.939+0000] {hive.py:475} INFO - USE `default`
[2024-12-14T11:27:30.960+0000] {hive.py:475} INFO - SELECT COUNT(*) FROM default.instruction_fact
[2024-12-14T11:27:31.131+0000] {logging_mixin.py:188} INFO - SELECT COUNT(*) FROM default.instruction_fact --- 939
[2024-12-14T11:27:31.134+0000] {hive.py:475} INFO - 
            MERGE INTO iceberg.warehouse.instruction_fact t
            USING default.instruction_fact s
            ON t.instruction_id=s.instruction_id
            WHEN MATCHED 
                THEN UPDATE SET 
                t.instruction_id=s.instruction_id,t.program_semester_id=s.program_semester_id,t.course_id=s.course_id,t.class_id=s.class_id,t.is_required=s.is_required,t.num_student=s.num_student,t.num_pass_student=s.num_pass_student,t.num_fail_student=s.num_fail_student,t.avg_final_score=s.avg_final_score,t.instruction_status=s.instruction_status,t.instruction_allocate=s.instruction_allocate,t.instruction_time_start=s.instruction_time_start,t.instruction_time_end=s.instruction_time_end,t.last_modified_date=s.last_modified_date,t.etl_date=s.etl_date
            WHEN NOT MATCHED 
                THEN INSERT *
        
[2024-12-14T11:27:33.177+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-12-14T11:27:33.178+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/tasks/fit_task.py", line 97, in insert_warehouse_table
    cursor.execute(f"""
  File "/home/airflow/.local/lib/python3.12/site-packages/pyhive/hive.py", line 481, in execute
    _check_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/pyhive/hive.py", line 611, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 349.0 failed 4 times, most recent failure: Lost task 0.3 in stage 349.0 (TID 281) (172.19.0.31 executor 0): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/fit_warehouse/raw/instruction_fact/instruction_fact_01.snappy.parquet. Column: [num_student], Expected: bigint, Found: DOUBLE.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [num_student], physicalType: DOUBLE, logicalType: bigint\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1127)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:189)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 21 more\n\nDriver stacktrace::36:35', 'org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$:runningQueryError:HiveThriftServerErrors.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:262', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:runInternal:SparkExecuteStatementOperation.scala:152', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:277', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkOperation$$super$run:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:$anonfun$run$1:SparkOperation.scala:45', 'scala.runtime.java8.JFunction0$mcV$sp:apply:JFunction0$mcV$sp.java:23', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties:SparkOperation.scala:79', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties$:SparkOperation.scala:63', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:withLocalProperties:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run:SparkOperation.scala:45', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run$:SparkOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:run:SparkExecuteStatementOperation.scala:41', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:484', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:460', 'jdk.internal.reflect.GeneratedMethodAccessor16:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke::-1', 'java.lang.reflect.Method:invoke::-1', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:71', 'org.apache.hive.service.cli.session.HiveSessionProxy:lambda$invoke$0:HiveSessionProxy.java:58', 'java.security.AccessController:doPrivileged::-2', 'javax.security.auth.Subject:doAs::-1', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:58', 'com.sun.proxy.$Proxy39:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:282', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:453', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:52', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:310', 'java.util.concurrent.ThreadPoolExecutor:runWorker::-1', 'java.util.concurrent.ThreadPoolExecutor$Worker:run::-1', 'java.lang.Thread:run::-1', '*org.apache.spark.SparkException:Job aborted due to stage failure: Task 0 in stage 349.0 failed 4 times, most recent failure: Lost task 0.3 in stage 349.0 (TID 281) (172.19.0.31 executor 0): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/fit_warehouse/raw/instruction_fact/instruction_fact_01.snappy.parquet. Column: [num_student], Expected: bigint, Found: DOUBLE.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [num_student], physicalType: DOUBLE, logicalType: bigint\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1127)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:189)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 21 more\n\nDriver stacktrace::15:14', 'org.apache.spark.scheduler.DAGScheduler:failJobAndIndependentStages:DAGScheduler.scala:2844', 'org.apache.spark.scheduler.DAGScheduler:$anonfun$abortStage$2:DAGScheduler.scala:2780', 'org.apache.spark.scheduler.DAGScheduler:$anonfun$abortStage$2$adapted:DAGScheduler.scala:2779', 'scala.collection.mutable.ResizableArray:foreach:ResizableArray.scala:62', 'scala.collection.mutable.ResizableArray:foreach$:ResizableArray.scala:55', 'scala.collection.mutable.ArrayBuffer:foreach:ArrayBuffer.scala:49', 'org.apache.spark.scheduler.DAGScheduler:abortStage:DAGScheduler.scala:2779', 'org.apache.spark.scheduler.DAGScheduler:$anonfun$handleTaskSetFailed$1:DAGScheduler.scala:1242', 'org.apache.spark.scheduler.DAGScheduler:$anonfun$handleTaskSetFailed$1$adapted:DAGScheduler.scala:1242', 'scala.Option:foreach:Option.scala:407', 'org.apache.spark.scheduler.DAGScheduler:handleTaskSetFailed:DAGScheduler.scala:1242', 'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop:doOnReceive:DAGScheduler.scala:3048', 'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop:onReceive:DAGScheduler.scala:2982', 'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop:onReceive:DAGScheduler.scala:2971', 'org.apache.spark.util.EventLoop$$anon$1:run:EventLoop.scala:49', '*org.apache.spark.SparkException:Parquet column cannot be converted in file hdfs://namenode:9000/fit_warehouse/raw/instruction_fact/instruction_fact_01.snappy.parquet. Column: [num_student], Expected: bigint, Found: DOUBLE.:23:22', 'org.apache.spark.sql.errors.QueryExecutionErrors$:unsupportedSchemaColumnConvertError:QueryExecutionErrors.scala:854', 'org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1:nextIterator:FileScanRDD.scala:287', 'org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1:hasNext:FileScanRDD.scala:129', 'org.apache.spark.sql.execution.FileSourceScanExec$$anon$1:hasNext:DataSourceScanExec.scala:593', 'org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2:columnartorow_nextBatch_0$::-1', 'org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2:processNext::-1', 'org.apache.spark.sql.execution.BufferedRowIterator:hasNext:BufferedRowIterator.java:43', 'org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1:hasNext:WholeStageCodegenEvaluatorFactory.scala:43', 'scala.collection.Iterator$$anon$10:hasNext:Iterator.scala:460', 'org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter:write:BypassMergeSortShuffleWriter.java:140', 'org.apache.spark.shuffle.ShuffleWriteProcessor:write:ShuffleWriteProcessor.scala:59', 'org.apache.spark.scheduler.ShuffleMapTask:runTask:ShuffleMapTask.scala:104', 'org.apache.spark.scheduler.ShuffleMapTask:runTask:ShuffleMapTask.scala:54', 'org.apache.spark.TaskContext:runTaskWithListeners:TaskContext.scala:161', 'org.apache.spark.scheduler.Task:run:Task.scala:141', 'org.apache.spark.executor.Executor$TaskRunner:$anonfun$run$4:Executor.scala:620', 'org.apache.spark.util.SparkErrorUtils:tryWithSafeFinally:SparkErrorUtils.scala:64', 'org.apache.spark.util.SparkErrorUtils:tryWithSafeFinally$:SparkErrorUtils.scala:61', 'org.apache.spark.util.Utils$:tryWithSafeFinally:Utils.scala:94', 'org.apache.spark.executor.Executor$TaskRunner:run:Executor.scala:623', 'java.util.concurrent.ThreadPoolExecutor:runWorker::-1', 'java.util.concurrent.ThreadPoolExecutor$Worker:run::-1', 'java.lang.Thread:run::-1', '*org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException:column: [num_student], physicalType: DOUBLE, logicalType: bigint:29:7', 'org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory:constructConvertNotSupportedException:ParquetVectorUpdaterFactory.java:1127', 'org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory:getUpdater:ParquetVectorUpdaterFactory.java:189', 'org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader:readBatch:VectorizedColumnReader.java:175', 'org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader:nextBatch:VectorizedParquetRecordReader.java:342', 'org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader:nextKeyValue:VectorizedParquetRecordReader.java:233', 'org.apache.spark.sql.execution.datasources.RecordReaderIterator:hasNext:RecordReaderIterator.scala:39', 'org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1:hasNext:FileScanRDD.scala:129', 'org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1:nextIterator:FileScanRDD.scala:283'], sqlState=None, errorCode=0, errorMessage='Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 349.0 failed 4 times, most recent failure: Lost task 0.3 in stage 349.0 (TID 281) (172.19.0.31 executor 0): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/fit_warehouse/raw/instruction_fact/instruction_fact_01.snappy.parquet. Column: [num_student], Expected: bigint, Found: DOUBLE.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [num_student], physicalType: DOUBLE, logicalType: bigint\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1127)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:189)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 21 more\n\nDriver stacktrace:'), operationHandle=None)
[2024-12-14T11:27:33.201+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=etl_instruction_fact_data, task_id=insert_warehouse.insert_dwh_instruction_fact, run_id=manual__2024-12-14T11:27:22.467792+00:00, execution_date=20241214T112722, start_date=20241214T112730, end_date=20241214T112733
[2024-12-14T11:27:33.220+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 165 for task insert_warehouse.insert_dwh_instruction_fact (TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 349.0 failed 4 times, most recent failure: Lost task 0.3 in stage 349.0 (TID 281) (172.19.0.31 executor 0): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/fit_warehouse/raw/instruction_fact/instruction_fact_01.snappy.parquet. Column: [num_student], Expected: bigint, Found: DOUBLE.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [num_student], physicalType: DOUBLE, logicalType: bigint\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1127)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:189)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 21 more\n\nDriver stacktrace::36:35', 'org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$:runningQueryError:HiveThriftServerErrors.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:262', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:runInternal:SparkExecuteStatementOperation.scala:152', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:277', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkOperation$$super$run:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:$anonfun$run$1:SparkOperation.scala:45', 'scala.runtime.java8.JFunction0$mcV$sp:apply:JFunction0$mcV$sp.java:23', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties:SparkOperation.scala:79', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties$:SparkOperation.scala:63', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:withLocalProperties:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run:SparkOperation.scala:45', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run$:SparkOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:run:SparkExecuteStatementOperation.scala:41', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:484', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:460', 'jdk.internal.reflect.GeneratedMethodAccessor16:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke::-1', 'java.lang.reflect.Method:invoke::-1', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:71', 'org.apache.hive.service.cli.session.HiveSessionProxy:lambda$invoke$0:HiveSessionProxy.java:58', 'java.security.AccessController:doPrivileged::-2', 'javax.security.auth.Subject:doAs::-1', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:58', 'com.sun.proxy.$Proxy39:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:282', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:453', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:52', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:310', 'java.util.concurrent.ThreadPoolExecutor:runWorker::-1', 'java.util.concurrent.ThreadPoolExecutor$Worker:run::-1', 'java.lang.Thread:run::-1', '*org.apache.spark.SparkException:Job aborted due to stage failure: Task 0 in stage 349.0 failed 4 times, most recent failure: Lost task 0.3 in stage 349.0 (TID 281) (172.19.0.31 executor 0): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/fit_warehouse/raw/instruction_fact/instruction_fact_01.snappy.parquet. Column: [num_student], Expected: bigint, Found: DOUBLE.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [num_student], physicalType: DOUBLE, logicalType: bigint\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1127)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:189)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 21 more\n\nDriver stacktrace::15:14', 'org.apache.spark.scheduler.DAGScheduler:failJobAndIndependentStages:DAGScheduler.scala:2844', 'org.apache.spark.scheduler.DAGScheduler:$anonfun$abortStage$2:DAGScheduler.scala:2780', 'org.apache.spark.scheduler.DAGScheduler:$anonfun$abortStage$2$adapted:DAGScheduler.scala:2779', 'scala.collection.mutable.ResizableArray:foreach:ResizableArray.scala:62', 'scala.collection.mutable.ResizableArray:foreach$:ResizableArray.scala:55', 'scala.collection.mutable.ArrayBuffer:foreach:ArrayBuffer.scala:49', 'org.apache.spark.scheduler.DAGScheduler:abortStage:DAGScheduler.scala:2779', 'org.apache.spark.scheduler.DAGScheduler:$anonfun$handleTaskSetFailed$1:DAGScheduler.scala:1242', 'org.apache.spark.scheduler.DAGScheduler:$anonfun$handleTaskSetFailed$1$adapted:DAGScheduler.scala:1242', 'scala.Option:foreach:Option.scala:407', 'org.apache.spark.scheduler.DAGScheduler:handleTaskSetFailed:DAGScheduler.scala:1242', 'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop:doOnReceive:DAGScheduler.scala:3048', 'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop:onReceive:DAGScheduler.scala:2982', 'org.apache.spark.scheduler.DAGSchedulerEventProcessLoop:onReceive:DAGScheduler.scala:2971', 'org.apache.spark.util.EventLoop$$anon$1:run:EventLoop.scala:49', '*org.apache.spark.SparkException:Parquet column cannot be converted in file hdfs://namenode:9000/fit_warehouse/raw/instruction_fact/instruction_fact_01.snappy.parquet. Column: [num_student], Expected: bigint, Found: DOUBLE.:23:22', 'org.apache.spark.sql.errors.QueryExecutionErrors$:unsupportedSchemaColumnConvertError:QueryExecutionErrors.scala:854', 'org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1:nextIterator:FileScanRDD.scala:287', 'org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1:hasNext:FileScanRDD.scala:129', 'org.apache.spark.sql.execution.FileSourceScanExec$$anon$1:hasNext:DataSourceScanExec.scala:593', 'org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2:columnartorow_nextBatch_0$::-1', 'org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2:processNext::-1', 'org.apache.spark.sql.execution.BufferedRowIterator:hasNext:BufferedRowIterator.java:43', 'org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1:hasNext:WholeStageCodegenEvaluatorFactory.scala:43', 'scala.collection.Iterator$$anon$10:hasNext:Iterator.scala:460', 'org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter:write:BypassMergeSortShuffleWriter.java:140', 'org.apache.spark.shuffle.ShuffleWriteProcessor:write:ShuffleWriteProcessor.scala:59', 'org.apache.spark.scheduler.ShuffleMapTask:runTask:ShuffleMapTask.scala:104', 'org.apache.spark.scheduler.ShuffleMapTask:runTask:ShuffleMapTask.scala:54', 'org.apache.spark.TaskContext:runTaskWithListeners:TaskContext.scala:161', 'org.apache.spark.scheduler.Task:run:Task.scala:141', 'org.apache.spark.executor.Executor$TaskRunner:$anonfun$run$4:Executor.scala:620', 'org.apache.spark.util.SparkErrorUtils:tryWithSafeFinally:SparkErrorUtils.scala:64', 'org.apache.spark.util.SparkErrorUtils:tryWithSafeFinally$:SparkErrorUtils.scala:61', 'org.apache.spark.util.Utils$:tryWithSafeFinally:Utils.scala:94', 'org.apache.spark.executor.Executor$TaskRunner:run:Executor.scala:623', 'java.util.concurrent.ThreadPoolExecutor:runWorker::-1', 'java.util.concurrent.ThreadPoolExecutor$Worker:run::-1', 'java.lang.Thread:run::-1', '*org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException:column: [num_student], physicalType: DOUBLE, logicalType: bigint:29:7', 'org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory:constructConvertNotSupportedException:ParquetVectorUpdaterFactory.java:1127', 'org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory:getUpdater:ParquetVectorUpdaterFactory.java:189', 'org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader:readBatch:VectorizedColumnReader.java:175', 'org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader:nextBatch:VectorizedParquetRecordReader.java:342', 'org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader:nextKeyValue:VectorizedParquetRecordReader.java:233', 'org.apache.spark.sql.execution.datasources.RecordReaderIterator:hasNext:RecordReaderIterator.scala:39', 'org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1:hasNext:FileScanRDD.scala:129', 'org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1:nextIterator:FileScanRDD.scala:283'], sqlState=None, errorCode=0, errorMessage='Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 349.0 failed 4 times, most recent failure: Lost task 0.3 in stage 349.0 (TID 281) (172.19.0.31 executor 0): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/fit_warehouse/raw/instruction_fact/instruction_fact_01.snappy.parquet. Column: [num_student], Expected: bigint, Found: DOUBLE.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [num_student], physicalType: DOUBLE, logicalType: bigint\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1127)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:189)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 21 more\n\nDriver stacktrace:'), operationHandle=None); 793)
[2024-12-14T11:27:33.250+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2024-12-14T11:27:33.297+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-12-14T11:27:33.304+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
