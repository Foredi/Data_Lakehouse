[2024-12-14T08:45:15.623+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-12-14T08:45:15.654+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_instruction_fact_data.insert_warehouse.insert_dwh_instruction_fact manual__2024-12-14T08:45:05.407559+00:00 [queued]>
[2024-12-14T08:45:15.662+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_instruction_fact_data.insert_warehouse.insert_dwh_instruction_fact manual__2024-12-14T08:45:05.407559+00:00 [queued]>
[2024-12-14T08:45:15.662+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-12-14T08:45:15.673+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): insert_warehouse.insert_dwh_instruction_fact> on 2024-12-14 08:45:05.407559+00:00
[2024-12-14T08:45:15.685+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=16154) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-12-14T08:45:15.687+0000] {standard_task_runner.py:63} INFO - Started process 16157 to run task
[2024-12-14T08:45:15.693+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'etl_instruction_fact_data', 'insert_warehouse.insert_dwh_instruction_fact', 'manual__2024-12-14T08:45:05.407559+00:00', '--job-id', '1092', '--raw', '--subdir', 'DAGS_FOLDER/elt_instruction_fact.py', '--cfg-path', '/tmp/tmpc5tuh_09']
[2024-12-14T08:45:15.695+0000] {standard_task_runner.py:91} INFO - Job 1092: Subtask insert_warehouse.insert_dwh_instruction_fact
[2024-12-14T08:45:15.759+0000] {task_command.py:426} INFO - Running <TaskInstance: etl_instruction_fact_data.insert_warehouse.insert_dwh_instruction_fact manual__2024-12-14T08:45:05.407559+00:00 [running]> on host 1fe973f28a1c
[2024-12-14T08:45:15.888+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='etl_instruction_fact_data' AIRFLOW_CTX_TASK_ID='insert_warehouse.insert_dwh_instruction_fact' AIRFLOW_CTX_EXECUTION_DATE='2024-12-14T08:45:05.407559+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-14T08:45:05.407559+00:00'
[2024-12-14T08:45:15.889+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-12-14T08:45:15.899+0000] {hive.py:475} INFO - USE `default`
[2024-12-14T08:45:15.935+0000] {hive.py:475} INFO - SELECT COUNT(*) FROM default.instruction_fact
[2024-12-14T08:45:16.323+0000] {logging_mixin.py:188} INFO - SELECT COUNT(*) FROM default.instruction_fact --- 441
[2024-12-14T08:45:16.327+0000] {hive.py:475} INFO - 
            MERGE INTO iceberg.warehouse.instruction_fact t
            USING default.instruction_fact s
            ON t.instruction_id=s.instruction_id
            WHEN MATCHED 
                THEN UPDATE SET 
                t.instruction_id=s.instruction_id,t.program_semester_id=s.program_semester_id,t.course_id=s.course_id,t.class_id=s.class_id,t.is_required=s.is_required,t.num_student=s.num_student,t.num_pass_student=s.num_pass_student,t.num_fail_student=s.num_fail_student,t.avg_final_score=s.avg_final_score,t.instruction_status=s.instruction_status,t.instruction_allocate=s.instruction_allocate,t.instruction_time_start=s.instruction_time_start,t.instruction_time_end=s.instruction_time_end,t.last_modified_date=s.last_modified_date,t.etl_date=s.etl_date
            WHEN NOT MATCHED 
                THEN INSERT *
        
[2024-12-14T08:45:16.515+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-12-14T08:45:16.516+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/tasks/fit_task.py", line 97, in insert_warehouse_table
    cursor.execute(f"""
  File "/home/airflow/.local/lib/python3.12/site-packages/pyhive/hive.py", line 481, in execute
    _check_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/pyhive/hive.py", line 611, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error running query: org.apache.iceberg.exceptions.NotFoundException: Failed to open input stream for file: hdfs://namenode:9000/fit_warehouse/warehouse/instruction_fact/*.snappy.parquet/metadata/00002-3c33c449-9e4e-4732-b7ab-fb7228589a5f.metadata.json:36:35', 'org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$:runningQueryError:HiveThriftServerErrors.scala:46', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:262', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:runInternal:SparkExecuteStatementOperation.scala:152', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:277', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkOperation$$super$run:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:$anonfun$run$1:SparkOperation.scala:45', 'scala.runtime.java8.JFunction0$mcV$sp:apply:JFunction0$mcV$sp.java:23', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties:SparkOperation.scala:79', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties$:SparkOperation.scala:63', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:withLocalProperties:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run:SparkOperation.scala:45', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run$:SparkOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:run:SparkExecuteStatementOperation.scala:41', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:484', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:460', 'jdk.internal.reflect.GeneratedMethodAccessor20:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke::-1', 'java.lang.reflect.Method:invoke::-1', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:71', 'org.apache.hive.service.cli.session.HiveSessionProxy:lambda$invoke$0:HiveSessionProxy.java:58', 'java.security.AccessController:doPrivileged::-2', 'javax.security.auth.Subject:doAs::-1', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:58', 'com.sun.proxy.$Proxy39:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:282', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:453', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:52', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:310', 'java.util.concurrent.ThreadPoolExecutor:runWorker::-1', 'java.util.concurrent.ThreadPoolExecutor$Worker:run::-1', 'java.lang.Thread:run::-1', '*org.apache.iceberg.exceptions.NotFoundException:Failed to open input stream for file: hdfs://namenode:9000/fit_warehouse/warehouse/instruction_fact/*.snappy.parquet/metadata/00002-3c33c449-9e4e-4732-b7ab-fb7228589a5f.metadata.json:134:99', 'org.apache.iceberg.hadoop.HadoopInputFile:newStream:HadoopInputFile.java:185', 'org.apache.iceberg.TableMetadataParser:read:TableMetadataParser.java:279', 'org.apache.iceberg.TableMetadataParser:read:TableMetadataParser.java:273', 'org.apache.iceberg.BaseMetastoreTableOperations:lambda$refreshFromMetadataLocation$0:BaseMetastoreTableOperations.java:189', 'org.apache.iceberg.BaseMetastoreTableOperations:lambda$refreshFromMetadataLocation$1:BaseMetastoreTableOperations.java:208', 'org.apache.iceberg.util.Tasks$Builder:runTaskWithRetry:Tasks.java:413', 'org.apache.iceberg.util.Tasks$Builder:runSingleThreaded:Tasks.java:219', 'org.apache.iceberg.util.Tasks$Builder:run:Tasks.java:203', 'org.apache.iceberg.util.Tasks$Builder:run:Tasks.java:196', 'org.apache.iceberg.BaseMetastoreTableOperations:refreshFromMetadataLocation:BaseMetastoreTableOperations.java:208', 'org.apache.iceberg.BaseMetastoreTableOperations:refreshFromMetadataLocation:BaseMetastoreTableOperations.java:185', 'org.apache.iceberg.BaseMetastoreTableOperations:refreshFromMetadataLocation:BaseMetastoreTableOperations.java:180', 'org.apache.iceberg.hive.HiveTableOperations:doRefresh:HiveTableOperations.java:166', 'org.apache.iceberg.BaseMetastoreTableOperations:refresh:BaseMetastoreTableOperations.java:97', 'org.apache.iceberg.BaseMetastoreTableOperations:current:BaseMetastoreTableOperations.java:80', 'org.apache.iceberg.BaseMetastoreCatalog:loadTable:BaseMetastoreCatalog.java:49', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache:lambda$doComputeIfAbsent$14:BoundedLocalCache.java:2406', 'java.util.concurrent.ConcurrentHashMap:compute::-1', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache:doComputeIfAbsent:BoundedLocalCache.java:2404', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache:computeIfAbsent:BoundedLocalCache.java:2387', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache:computeIfAbsent:LocalCache.java:108', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache:get:LocalManualCache.java:62', 'org.apache.iceberg.CachingCatalog:loadTable:CachingCatalog.java:166', 'org.apache.iceberg.spark.SparkCatalog:load:SparkCatalog.java:843', 'org.apache.iceberg.spark.SparkCatalog:loadTable:SparkCatalog.java:170', 'org.apache.spark.sql.connector.catalog.CatalogV2Util$:getTable:CatalogV2Util.scala:355', 'org.apache.spark.sql.connector.catalog.CatalogV2Util$:loadTable:CatalogV2Util.scala:336', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:$anonfun$resolveRelation$3:Analyzer.scala:1268', 'scala.Option:orElse:Option.scala:447', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:$anonfun$resolveRelation$1:Analyzer.scala:1267', 'scala.Option:orElse:Option.scala:447', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation:Analyzer.scala:1259', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14:applyOrElse:Analyzer.scala:1123', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14:applyOrElse:Analyzer.scala:1087', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$3:AnalysisHelper.scala:138', 'org.apache.spark.sql.catalyst.trees.CurrentOrigin$:withOrigin:origin.scala:76', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:138', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:32', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren:TreeNode.scala:1215', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren$:TreeNode.scala:1214', 'org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias:mapChildren:basicLogicalOperators.scala:1659', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:32', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.BinaryLike:mapChildren:TreeNode.scala:1241', 'org.apache.spark.sql.catalyst.trees.BinaryLike:mapChildren$:TreeNode.scala:1240', 'org.apache.spark.sql.catalyst.plans.logical.MergeIntoTable:mapChildren:v2Commands.scala:716', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:32', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:apply:Analyzer.scala:1087', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:apply:Analyzer.scala:1046', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$2:RuleExecutor.scala:222', 'scala.collection.LinearSeqOptimized:foldLeft:LinearSeqOptimized.scala:126', 'scala.collection.LinearSeqOptimized:foldLeft$:LinearSeqOptimized.scala:122', 'scala.collection.immutable.List:foldLeft:List.scala:91', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1:RuleExecutor.scala:219', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1$adapted:RuleExecutor.scala:211', 'scala.collection.immutable.List:foreach:List.scala:431', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:execute:RuleExecutor.scala:211', 'org.apache.spark.sql.catalyst.analysis.Analyzer:org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext:Analyzer.scala:226', 'org.apache.spark.sql.catalyst.analysis.Analyzer:$anonfun$execute$1:Analyzer.scala:222', 'org.apache.spark.sql.catalyst.analysis.AnalysisContext$:withNewAnalysisContext:Analyzer.scala:173', 'org.apache.spark.sql.catalyst.analysis.Analyzer:execute:Analyzer.scala:222', 'org.apache.spark.sql.catalyst.analysis.Analyzer:execute:Analyzer.scala:188', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$executeAndTrack$1:RuleExecutor.scala:182', 'org.apache.spark.sql.catalyst.QueryPlanningTracker$:withTracker:QueryPlanningTracker.scala:89', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:executeAndTrack:RuleExecutor.scala:182', 'org.apache.spark.sql.catalyst.analysis.Analyzer:$anonfun$executeAndCheck$1:Analyzer.scala:209', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:markInAnalyzer:AnalysisHelper.scala:330', 'org.apache.spark.sql.catalyst.analysis.Analyzer:executeAndCheck:Analyzer.scala:208', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$analyzed$1:QueryExecution.scala:77', 'org.apache.spark.sql.catalyst.QueryPlanningTracker:measurePhase:QueryPlanningTracker.scala:138', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$2:QueryExecution.scala:219', 'org.apache.spark.sql.execution.QueryExecution$:withInternalError:QueryExecution.scala:546', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$1:QueryExecution.scala:219', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.execution.QueryExecution:executePhase:QueryExecution.scala:218', 'org.apache.spark.sql.execution.QueryExecution:analyzed$lzycompute:QueryExecution.scala:77', 'org.apache.spark.sql.execution.QueryExecution:analyzed:QueryExecution.scala:74', 'org.apache.spark.sql.execution.QueryExecution:assertAnalyzed:QueryExecution.scala:66', 'org.apache.spark.sql.Dataset$:$anonfun$ofRows$2:Dataset.scala:99', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.Dataset$:ofRows:Dataset.scala:97', 'org.apache.spark.sql.SparkSession:$anonfun$sql$4:SparkSession.scala:691', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:682', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:713', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:744', 'org.apache.spark.sql.SQLContext:sql:SQLContext.scala:651', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:227', '*java.io.FileNotFoundException:File does not exist: /fit_warehouse/warehouse/instruction_fact/*.snappy.parquet/metadata/00002-3c33c449-9e4e-4732-b7ab-fb7228589a5f.metadata.json\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2089)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:762)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:458)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n:149:15', 'jdk.internal.reflect.NativeConstructorAccessorImpl:newInstance0::-2', 'jdk.internal.reflect.NativeConstructorAccessorImpl:newInstance::-1', 'jdk.internal.reflect.DelegatingConstructorAccessorImpl:newInstance::-1', 'java.lang.reflect.Constructor:newInstance::-1', 'org.apache.hadoop.ipc.RemoteException:instantiateException:RemoteException.java:121', 'org.apache.hadoop.ipc.RemoteException:unwrapRemoteException:RemoteException.java:88', 'org.apache.hadoop.hdfs.DFSClient:callGetBlockLocations:DFSClient.java:902', 'org.apache.hadoop.hdfs.DFSClient:getLocatedBlocks:DFSClient.java:889', 'org.apache.hadoop.hdfs.DFSClient:getLocatedBlocks:DFSClient.java:878', 'org.apache.hadoop.hdfs.DFSClient:open:DFSClient.java:1046', 'org.apache.hadoop.hdfs.DistributedFileSystem$4:doCall:DistributedFileSystem.java:340', 'org.apache.hadoop.hdfs.DistributedFileSystem$4:doCall:DistributedFileSystem.java:336', 'org.apache.hadoop.fs.FileSystemLinkResolver:resolve:FileSystemLinkResolver.java:81', 'org.apache.hadoop.hdfs.DistributedFileSystem:open:DistributedFileSystem.java:353', 'org.apache.hadoop.fs.FileSystem:open:FileSystem.java:976', 'org.apache.iceberg.hadoop.HadoopInputFile:newStream:HadoopInputFile.java:183', '*org.apache.hadoop.ipc.RemoteException:File does not exist: /fit_warehouse/warehouse/instruction_fact/*.snappy.parquet/metadata/00002-3c33c449-9e4e-4732-b7ab-fb7228589a5f.metadata.json\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2089)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:762)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:458)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n:159:16', 'org.apache.hadoop.ipc.Client:getRpcResponse:Client.java:1612', 'org.apache.hadoop.ipc.Client:call:Client.java:1558', 'org.apache.hadoop.ipc.Client:call:Client.java:1455', 'org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke:ProtobufRpcEngine2.java:242', 'org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke:ProtobufRpcEngine2.java:129', 'com.sun.proxy.$Proxy34:getBlockLocations::-1', 'org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB:getBlockLocations:ClientNamenodeProtocolTranslatorPB.java:333', 'jdk.internal.reflect.GeneratedMethodAccessor80:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke::-1', 'java.lang.reflect.Method:invoke::-1', 'org.apache.hadoop.io.retry.RetryInvocationHandler:invokeMethod:RetryInvocationHandler.java:422', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeMethod:RetryInvocationHandler.java:165', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invoke:RetryInvocationHandler.java:157', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeOnce:RetryInvocationHandler.java:95', 'org.apache.hadoop.io.retry.RetryInvocationHandler:invoke:RetryInvocationHandler.java:359', 'com.sun.proxy.$Proxy35:getBlockLocations::-1', 'org.apache.hadoop.hdfs.DFSClient:callGetBlockLocations:DFSClient.java:900'], sqlState=None, errorCode=0, errorMessage='Error running query: org.apache.iceberg.exceptions.NotFoundException: Failed to open input stream for file: hdfs://namenode:9000/fit_warehouse/warehouse/instruction_fact/*.snappy.parquet/metadata/00002-3c33c449-9e4e-4732-b7ab-fb7228589a5f.metadata.json'), operationHandle=None)
[2024-12-14T08:45:16.534+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=etl_instruction_fact_data, task_id=insert_warehouse.insert_dwh_instruction_fact, run_id=manual__2024-12-14T08:45:05.407559+00:00, execution_date=20241214T084505, start_date=20241214T084515, end_date=20241214T084516
[2024-12-14T08:45:16.551+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 1092 for task insert_warehouse.insert_dwh_instruction_fact (TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error running query: org.apache.iceberg.exceptions.NotFoundException: Failed to open input stream for file: hdfs://namenode:9000/fit_warehouse/warehouse/instruction_fact/*.snappy.parquet/metadata/00002-3c33c449-9e4e-4732-b7ab-fb7228589a5f.metadata.json:36:35', 'org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$:runningQueryError:HiveThriftServerErrors.scala:46', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:262', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:runInternal:SparkExecuteStatementOperation.scala:152', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:277', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkOperation$$super$run:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:$anonfun$run$1:SparkOperation.scala:45', 'scala.runtime.java8.JFunction0$mcV$sp:apply:JFunction0$mcV$sp.java:23', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties:SparkOperation.scala:79', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:withLocalProperties$:SparkOperation.scala:63', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:withLocalProperties:SparkExecuteStatementOperation.scala:41', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run:SparkOperation.scala:45', 'org.apache.spark.sql.hive.thriftserver.SparkOperation:run$:SparkOperation.scala:43', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:run:SparkExecuteStatementOperation.scala:41', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:484', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:460', 'jdk.internal.reflect.GeneratedMethodAccessor20:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke::-1', 'java.lang.reflect.Method:invoke::-1', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:71', 'org.apache.hive.service.cli.session.HiveSessionProxy:lambda$invoke$0:HiveSessionProxy.java:58', 'java.security.AccessController:doPrivileged::-2', 'javax.security.auth.Subject:doAs::-1', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1878', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:58', 'com.sun.proxy.$Proxy39:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:282', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:453', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:52', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:310', 'java.util.concurrent.ThreadPoolExecutor:runWorker::-1', 'java.util.concurrent.ThreadPoolExecutor$Worker:run::-1', 'java.lang.Thread:run::-1', '*org.apache.iceberg.exceptions.NotFoundException:Failed to open input stream for file: hdfs://namenode:9000/fit_warehouse/warehouse/instruction_fact/*.snappy.parquet/metadata/00002-3c33c449-9e4e-4732-b7ab-fb7228589a5f.metadata.json:134:99', 'org.apache.iceberg.hadoop.HadoopInputFile:newStream:HadoopInputFile.java:185', 'org.apache.iceberg.TableMetadataParser:read:TableMetadataParser.java:279', 'org.apache.iceberg.TableMetadataParser:read:TableMetadataParser.java:273', 'org.apache.iceberg.BaseMetastoreTableOperations:lambda$refreshFromMetadataLocation$0:BaseMetastoreTableOperations.java:189', 'org.apache.iceberg.BaseMetastoreTableOperations:lambda$refreshFromMetadataLocation$1:BaseMetastoreTableOperations.java:208', 'org.apache.iceberg.util.Tasks$Builder:runTaskWithRetry:Tasks.java:413', 'org.apache.iceberg.util.Tasks$Builder:runSingleThreaded:Tasks.java:219', 'org.apache.iceberg.util.Tasks$Builder:run:Tasks.java:203', 'org.apache.iceberg.util.Tasks$Builder:run:Tasks.java:196', 'org.apache.iceberg.BaseMetastoreTableOperations:refreshFromMetadataLocation:BaseMetastoreTableOperations.java:208', 'org.apache.iceberg.BaseMetastoreTableOperations:refreshFromMetadataLocation:BaseMetastoreTableOperations.java:185', 'org.apache.iceberg.BaseMetastoreTableOperations:refreshFromMetadataLocation:BaseMetastoreTableOperations.java:180', 'org.apache.iceberg.hive.HiveTableOperations:doRefresh:HiveTableOperations.java:166', 'org.apache.iceberg.BaseMetastoreTableOperations:refresh:BaseMetastoreTableOperations.java:97', 'org.apache.iceberg.BaseMetastoreTableOperations:current:BaseMetastoreTableOperations.java:80', 'org.apache.iceberg.BaseMetastoreCatalog:loadTable:BaseMetastoreCatalog.java:49', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache:lambda$doComputeIfAbsent$14:BoundedLocalCache.java:2406', 'java.util.concurrent.ConcurrentHashMap:compute::-1', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache:doComputeIfAbsent:BoundedLocalCache.java:2404', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache:computeIfAbsent:BoundedLocalCache.java:2387', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache:computeIfAbsent:LocalCache.java:108', 'org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache:get:LocalManualCache.java:62', 'org.apache.iceberg.CachingCatalog:loadTable:CachingCatalog.java:166', 'org.apache.iceberg.spark.SparkCatalog:load:SparkCatalog.java:843', 'org.apache.iceberg.spark.SparkCatalog:loadTable:SparkCatalog.java:170', 'org.apache.spark.sql.connector.catalog.CatalogV2Util$:getTable:CatalogV2Util.scala:355', 'org.apache.spark.sql.connector.catalog.CatalogV2Util$:loadTable:CatalogV2Util.scala:336', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:$anonfun$resolveRelation$3:Analyzer.scala:1268', 'scala.Option:orElse:Option.scala:447', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:$anonfun$resolveRelation$1:Analyzer.scala:1267', 'scala.Option:orElse:Option.scala:447', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation:Analyzer.scala:1259', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14:applyOrElse:Analyzer.scala:1123', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14:applyOrElse:Analyzer.scala:1087', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$3:AnalysisHelper.scala:138', 'org.apache.spark.sql.catalyst.trees.CurrentOrigin$:withOrigin:origin.scala:76', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:138', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:32', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren:TreeNode.scala:1215', 'org.apache.spark.sql.catalyst.trees.UnaryLike:mapChildren$:TreeNode.scala:1214', 'org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias:mapChildren:basicLogicalOperators.scala:1659', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:32', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$2:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.trees.BinaryLike:mapChildren:TreeNode.scala:1241', 'org.apache.spark.sql.catalyst.trees.BinaryLike:mapChildren$:TreeNode.scala:1240', 'org.apache.spark.sql.catalyst.plans.logical.MergeIntoTable:mapChildren:v2Commands.scala:716', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:$anonfun$resolveOperatorsUpWithPruning$1:AnalysisHelper.scala:135', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:allowInvokingTransformsInAnalyzer:AnalysisHelper.scala:323', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning:AnalysisHelper.scala:134', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper:resolveOperatorsUpWithPruning$:AnalysisHelper.scala:130', 'org.apache.spark.sql.catalyst.plans.logical.LogicalPlan:resolveOperatorsUpWithPruning:LogicalPlan.scala:32', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:apply:Analyzer.scala:1087', 'org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$:apply:Analyzer.scala:1046', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$2:RuleExecutor.scala:222', 'scala.collection.LinearSeqOptimized:foldLeft:LinearSeqOptimized.scala:126', 'scala.collection.LinearSeqOptimized:foldLeft$:LinearSeqOptimized.scala:122', 'scala.collection.immutable.List:foldLeft:List.scala:91', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1:RuleExecutor.scala:219', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$execute$1$adapted:RuleExecutor.scala:211', 'scala.collection.immutable.List:foreach:List.scala:431', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:execute:RuleExecutor.scala:211', 'org.apache.spark.sql.catalyst.analysis.Analyzer:org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext:Analyzer.scala:226', 'org.apache.spark.sql.catalyst.analysis.Analyzer:$anonfun$execute$1:Analyzer.scala:222', 'org.apache.spark.sql.catalyst.analysis.AnalysisContext$:withNewAnalysisContext:Analyzer.scala:173', 'org.apache.spark.sql.catalyst.analysis.Analyzer:execute:Analyzer.scala:222', 'org.apache.spark.sql.catalyst.analysis.Analyzer:execute:Analyzer.scala:188', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:$anonfun$executeAndTrack$1:RuleExecutor.scala:182', 'org.apache.spark.sql.catalyst.QueryPlanningTracker$:withTracker:QueryPlanningTracker.scala:89', 'org.apache.spark.sql.catalyst.rules.RuleExecutor:executeAndTrack:RuleExecutor.scala:182', 'org.apache.spark.sql.catalyst.analysis.Analyzer:$anonfun$executeAndCheck$1:Analyzer.scala:209', 'org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$:markInAnalyzer:AnalysisHelper.scala:330', 'org.apache.spark.sql.catalyst.analysis.Analyzer:executeAndCheck:Analyzer.scala:208', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$analyzed$1:QueryExecution.scala:77', 'org.apache.spark.sql.catalyst.QueryPlanningTracker:measurePhase:QueryPlanningTracker.scala:138', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$2:QueryExecution.scala:219', 'org.apache.spark.sql.execution.QueryExecution$:withInternalError:QueryExecution.scala:546', 'org.apache.spark.sql.execution.QueryExecution:$anonfun$executePhase$1:QueryExecution.scala:219', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.execution.QueryExecution:executePhase:QueryExecution.scala:218', 'org.apache.spark.sql.execution.QueryExecution:analyzed$lzycompute:QueryExecution.scala:77', 'org.apache.spark.sql.execution.QueryExecution:analyzed:QueryExecution.scala:74', 'org.apache.spark.sql.execution.QueryExecution:assertAnalyzed:QueryExecution.scala:66', 'org.apache.spark.sql.Dataset$:$anonfun$ofRows$2:Dataset.scala:99', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.Dataset$:ofRows:Dataset.scala:97', 'org.apache.spark.sql.SparkSession:$anonfun$sql$4:SparkSession.scala:691', 'org.apache.spark.sql.SparkSession:withActive:SparkSession.scala:900', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:682', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:713', 'org.apache.spark.sql.SparkSession:sql:SparkSession.scala:744', 'org.apache.spark.sql.SQLContext:sql:SQLContext.scala:651', 'org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute:SparkExecuteStatementOperation.scala:227', '*java.io.FileNotFoundException:File does not exist: /fit_warehouse/warehouse/instruction_fact/*.snappy.parquet/metadata/00002-3c33c449-9e4e-4732-b7ab-fb7228589a5f.metadata.json\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2089)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:762)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:458)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n:149:15', 'jdk.internal.reflect.NativeConstructorAccessorImpl:newInstance0::-2', 'jdk.internal.reflect.NativeConstructorAccessorImpl:newInstance::-1', 'jdk.internal.reflect.DelegatingConstructorAccessorImpl:newInstance::-1', 'java.lang.reflect.Constructor:newInstance::-1', 'org.apache.hadoop.ipc.RemoteException:instantiateException:RemoteException.java:121', 'org.apache.hadoop.ipc.RemoteException:unwrapRemoteException:RemoteException.java:88', 'org.apache.hadoop.hdfs.DFSClient:callGetBlockLocations:DFSClient.java:902', 'org.apache.hadoop.hdfs.DFSClient:getLocatedBlocks:DFSClient.java:889', 'org.apache.hadoop.hdfs.DFSClient:getLocatedBlocks:DFSClient.java:878', 'org.apache.hadoop.hdfs.DFSClient:open:DFSClient.java:1046', 'org.apache.hadoop.hdfs.DistributedFileSystem$4:doCall:DistributedFileSystem.java:340', 'org.apache.hadoop.hdfs.DistributedFileSystem$4:doCall:DistributedFileSystem.java:336', 'org.apache.hadoop.fs.FileSystemLinkResolver:resolve:FileSystemLinkResolver.java:81', 'org.apache.hadoop.hdfs.DistributedFileSystem:open:DistributedFileSystem.java:353', 'org.apache.hadoop.fs.FileSystem:open:FileSystem.java:976', 'org.apache.iceberg.hadoop.HadoopInputFile:newStream:HadoopInputFile.java:183', '*org.apache.hadoop.ipc.RemoteException:File does not exist: /fit_warehouse/warehouse/instruction_fact/*.snappy.parquet/metadata/00002-3c33c449-9e4e-4732-b7ab-fb7228589a5f.metadata.json\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2089)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:762)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:458)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)\n:159:16', 'org.apache.hadoop.ipc.Client:getRpcResponse:Client.java:1612', 'org.apache.hadoop.ipc.Client:call:Client.java:1558', 'org.apache.hadoop.ipc.Client:call:Client.java:1455', 'org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke:ProtobufRpcEngine2.java:242', 'org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke:ProtobufRpcEngine2.java:129', 'com.sun.proxy.$Proxy34:getBlockLocations::-1', 'org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB:getBlockLocations:ClientNamenodeProtocolTranslatorPB.java:333', 'jdk.internal.reflect.GeneratedMethodAccessor80:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke::-1', 'java.lang.reflect.Method:invoke::-1', 'org.apache.hadoop.io.retry.RetryInvocationHandler:invokeMethod:RetryInvocationHandler.java:422', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeMethod:RetryInvocationHandler.java:165', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invoke:RetryInvocationHandler.java:157', 'org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeOnce:RetryInvocationHandler.java:95', 'org.apache.hadoop.io.retry.RetryInvocationHandler:invoke:RetryInvocationHandler.java:359', 'com.sun.proxy.$Proxy35:getBlockLocations::-1', 'org.apache.hadoop.hdfs.DFSClient:callGetBlockLocations:DFSClient.java:900'], sqlState=None, errorCode=0, errorMessage='Error running query: org.apache.iceberg.exceptions.NotFoundException: Failed to open input stream for file: hdfs://namenode:9000/fit_warehouse/warehouse/instruction_fact/*.snappy.parquet/metadata/00002-3c33c449-9e4e-4732-b7ab-fb7228589a5f.metadata.json'), operationHandle=None); 16157)
[2024-12-14T08:45:16.572+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2024-12-14T08:45:16.753+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-12-14T08:45:16.765+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
